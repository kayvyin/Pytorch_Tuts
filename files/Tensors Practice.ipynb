{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fc3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f557bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3) #creates a 1-D tensor, with 3 elements = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f34c383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86dd28e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d6366a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b635fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874a1990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points  = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb369c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape #returns the size of the tensor along each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d18e164e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d05250ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f8158e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5b4e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3,5,5) #randomize a 3-D tensor to rep. an image [channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4d567b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2311, -0.5648,  1.4510,  0.0817,  0.6521],\n",
       "         [ 1.0340, -0.8841, -0.1853, -0.6256,  0.1557],\n",
       "         [ 1.2409,  0.7375, -0.2935,  0.2010,  0.2761],\n",
       "         [-0.4670, -0.1369, -1.5420, -0.1492, -0.0528],\n",
       "         [-0.6923, -0.1318,  2.5465, -0.8692,  0.1115]],\n",
       "\n",
       "        [[ 0.4744,  0.1000, -2.0015,  1.5330, -0.2539],\n",
       "         [-0.7702, -0.1202,  1.0854,  2.3698,  0.3709],\n",
       "         [ 1.7170,  0.6784, -1.0531, -0.6095,  0.8780],\n",
       "         [ 0.6619, -1.1325, -0.3882,  1.1156,  1.8449],\n",
       "         [ 0.3957, -0.7372, -0.1209, -0.7772,  1.4620]],\n",
       "\n",
       "        [[ 1.8854, -0.0298,  0.6059, -1.1886, -0.1295],\n",
       "         [-0.0726,  1.0048,  2.7486, -2.8614, -0.3658],\n",
       "         [ 0.9774,  1.4734,  0.5707,  2.4209, -0.9518],\n",
       "         [ 0.4847, -1.1413,  0.8214,  0.9159,  1.6774],\n",
       "         [ 1.4716, -0.3044,  0.6424,  0.0199,  0.8606]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t #Going to do a gray scale operation on this pretend image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd96abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([.2126, .7152, .0722]) #typical weights for colors to gray-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63fdf2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1913, -0.4238, -0.2389,  0.6240, -0.2423],\n",
       "          [-0.2585,  1.2012, -0.1477, -0.0259, -0.8374],\n",
       "          [-0.3875,  0.1393,  0.0232, -0.5663,  0.4538],\n",
       "          [-0.2990, -0.7492, -0.3243,  1.4963,  0.1243],\n",
       "          [ 0.7621,  0.8209,  1.2936,  0.0939,  0.5314]],\n",
       "\n",
       "         [[-1.7524, -0.8460, -1.0776,  1.2673,  0.4622],\n",
       "          [-0.7882, -1.1751,  0.0309,  0.6071, -1.5326],\n",
       "          [ 1.8705,  0.3437, -0.4009, -0.6207,  1.4118],\n",
       "          [-1.0742,  0.6929, -1.4886, -0.9399,  1.4520],\n",
       "          [ 2.1534,  0.2852,  0.1541, -1.0691, -0.4026]],\n",
       "\n",
       "         [[-0.7827,  0.0851,  1.2279, -0.9361,  0.0432],\n",
       "          [ 1.1040, -2.4619,  0.8498,  0.0804, -0.0322],\n",
       "          [-0.4668,  0.6177, -0.0523, -0.8248, -0.4563],\n",
       "          [ 0.6435, -0.5546, -0.9019,  1.1295, -0.4871],\n",
       "          [-0.9095, -0.2502,  0.8673,  0.2260,  0.0635]]],\n",
       "\n",
       "\n",
       "        [[[-0.0327,  0.3964, -0.3040, -1.6834,  0.2518],\n",
       "          [ 0.0982,  3.0025, -0.1698,  0.2657, -0.7968],\n",
       "          [-1.3685, -1.0963, -0.5637,  0.9901, -2.3058],\n",
       "          [-0.4801, -2.2432,  0.5655, -0.3318, -2.5465],\n",
       "          [-1.0660, -0.1613,  0.0168, -1.0585, -0.9136]],\n",
       "\n",
       "         [[-2.3273,  0.3532,  0.8218,  1.2953,  2.7834],\n",
       "          [ 0.2152, -0.5961,  0.8597, -0.9759, -0.2895],\n",
       "          [ 0.5747,  0.0296,  1.0429, -0.7124, -1.3934],\n",
       "          [ 1.0990,  1.5573, -1.3432, -0.7105, -1.0395],\n",
       "          [ 1.8815,  0.3941,  0.8159, -0.4574, -1.3289]],\n",
       "\n",
       "         [[ 0.2461, -0.7158,  0.1107,  1.4473,  0.8799],\n",
       "          [-1.3414,  0.1587, -1.2330, -0.5433,  0.3211],\n",
       "          [-1.4811,  0.5081, -1.2097,  0.0808, -0.2965],\n",
       "          [ 0.2721, -0.6052,  0.1220, -1.5185, -0.3748],\n",
       "          [ 0.9190, -0.8115,  0.7094, -0.9806, -0.0292]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) #shape [ batch, channels, rows, columns] (batch = # of samples in CNN)\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f43965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find unweighted means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e7562ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_naive = img_t.mean(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef95d0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.6366e-01, -1.6488e-01,  1.8447e-02,  1.4201e-01,  8.9588e-02],\n",
       "        [ 6.3735e-02,  1.7933e-04,  1.2163e+00, -3.7239e-01,  5.3598e-02],\n",
       "        [ 1.3118e+00,  9.6309e-01, -2.5864e-01,  6.7080e-01,  6.7431e-02],\n",
       "        [ 2.2653e-01, -8.0357e-01, -3.6957e-01,  6.2743e-01,  1.1565e+00],\n",
       "        [ 3.9168e-01, -3.9115e-01,  1.0227e+00, -5.4216e-01,  8.1135e-01]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive #dimension that is averaged is reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcba36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gray_naive = batch_t.mean(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6518b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2222, -0.4043,  0.1918,  0.3448, -0.1908],\n",
       "         [ 0.0105,  0.1389,  0.1906,  0.2511,  0.6440],\n",
       "         [-0.0163,  0.3174, -0.8774, -0.0171,  0.3230],\n",
       "         [-0.8098,  0.0055,  0.3726,  1.0297, -0.0661],\n",
       "         [ 0.4821, -0.5608,  0.2436,  0.2273,  0.8575]],\n",
       "\n",
       "        [[ 0.2359,  0.0477, -0.4360,  0.3114, -0.2479],\n",
       "         [-0.9193, -0.5521,  0.1219,  0.3002,  1.2946],\n",
       "         [-1.1939,  0.6218,  0.0042,  0.7718,  0.5895],\n",
       "         [ 0.0878, -0.4120, -0.1931,  0.9453, -0.6639],\n",
       "         [ 0.3743, -0.1028,  0.4058, -0.7099, -0.4617]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e80a3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can multiple tensors that are the same shape, as well as shapes that have size 1 in a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ed0c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsqueezing means we append a dimension w/ size 1, this is so product retains shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "656b9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d91d6484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2126]],\n",
       "\n",
       "        [[0.7152]],\n",
       "\n",
       "        [[0.0722]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7fdb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_weights = (img_t *unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ec3bd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.9129e-02, -1.2008e-01,  3.0847e-01,  1.7362e-02,  1.3864e-01],\n",
       "         [ 2.1983e-01, -1.8796e-01, -3.9386e-02, -1.3301e-01,  3.3092e-02],\n",
       "         [ 2.6383e-01,  1.5678e-01, -6.2403e-02,  4.2730e-02,  5.8695e-02],\n",
       "         [-9.9290e-02, -2.9115e-02, -3.2782e-01, -3.1725e-02, -1.1215e-02],\n",
       "         [-1.4718e-01, -2.8026e-02,  5.4139e-01, -1.8480e-01,  2.3705e-02]],\n",
       "\n",
       "        [[ 3.3932e-01,  7.1526e-02, -1.4315e+00,  1.0964e+00, -1.8158e-01],\n",
       "         [-5.5084e-01, -8.5961e-02,  7.7631e-01,  1.6949e+00,  2.6526e-01],\n",
       "         [ 1.2280e+00,  4.8518e-01, -7.5315e-01, -4.3591e-01,  6.2794e-01],\n",
       "         [ 4.7337e-01, -8.0993e-01, -2.7761e-01,  7.9791e-01,  1.3195e+00],\n",
       "         [ 2.8303e-01, -5.2727e-01, -8.6453e-02, -5.5584e-01,  1.0456e+00]],\n",
       "\n",
       "        [[ 1.3613e-01, -2.1540e-03,  4.3748e-02, -8.5818e-02, -9.3487e-03],\n",
       "         [-5.2409e-03,  7.2548e-02,  1.9845e-01, -2.0659e-01, -2.6407e-02],\n",
       "         [ 7.0569e-02,  1.0638e-01,  4.1202e-02,  1.7479e-01, -6.8719e-02],\n",
       "         [ 3.4999e-02, -8.2402e-02,  5.9306e-02,  6.6125e-02,  1.2110e-01],\n",
       "         [ 1.0625e-01, -2.1977e-02,  4.6379e-02,  1.4380e-03,  6.2135e-02]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c462405",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_weights = (batch_t * unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d176e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2533, -0.0901, -0.0508,  0.1327, -0.0515],\n",
       "          [-0.0550,  0.2554, -0.0314, -0.0055, -0.1780],\n",
       "          [-0.0824,  0.0296,  0.0049, -0.1204,  0.0965],\n",
       "          [-0.0636, -0.1593, -0.0690,  0.3181,  0.0264],\n",
       "          [ 0.1620,  0.1745,  0.2750,  0.0200,  0.1130]],\n",
       "\n",
       "         [[-1.2533, -0.6051, -0.7707,  0.9064,  0.3306],\n",
       "          [-0.5637, -0.8404,  0.0221,  0.4342, -1.0961],\n",
       "          [ 1.3378,  0.2458, -0.2868, -0.4439,  1.0097],\n",
       "          [-0.7683,  0.4956, -1.0646, -0.6722,  1.0385],\n",
       "          [ 1.5401,  0.2040,  0.1102, -0.7647, -0.2879]],\n",
       "\n",
       "         [[-0.0565,  0.0061,  0.0887, -0.0676,  0.0031],\n",
       "          [ 0.0797, -0.1777,  0.0614,  0.0058, -0.0023],\n",
       "          [-0.0337,  0.0446, -0.0038, -0.0596, -0.0329],\n",
       "          [ 0.0465, -0.0400, -0.0651,  0.0816, -0.0352],\n",
       "          [-0.0657, -0.0181,  0.0626,  0.0163,  0.0046]]],\n",
       "\n",
       "\n",
       "        [[[-0.0070,  0.0843, -0.0646, -0.3579,  0.0535],\n",
       "          [ 0.0209,  0.6383, -0.0361,  0.0565, -0.1694],\n",
       "          [-0.2910, -0.2331, -0.1198,  0.2105, -0.4902],\n",
       "          [-0.1021, -0.4769,  0.1202, -0.0705, -0.5414],\n",
       "          [-0.2266, -0.0343,  0.0036, -0.2250, -0.1942]],\n",
       "\n",
       "         [[-1.6645,  0.2526,  0.5878,  0.9264,  1.9907],\n",
       "          [ 0.1539, -0.4263,  0.6149, -0.6979, -0.2071],\n",
       "          [ 0.4110,  0.0211,  0.7459, -0.5095, -0.9966],\n",
       "          [ 0.7860,  1.1138, -0.9606, -0.5081, -0.7435],\n",
       "          [ 1.3456,  0.2819,  0.5835, -0.3271, -0.9504]],\n",
       "\n",
       "         [[ 0.0178, -0.0517,  0.0080,  0.1045,  0.0635],\n",
       "          [-0.0968,  0.0115, -0.0890, -0.0392,  0.0232],\n",
       "          [-0.1069,  0.0367, -0.0873,  0.0058, -0.0214],\n",
       "          [ 0.0196, -0.0437,  0.0088, -0.1096, -0.0271],\n",
       "          [ 0.0664, -0.0586,  0.0512, -0.0708, -0.0021]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24dcd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e12a90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gray_weighted = batch_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc585f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3775b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#^^ all this matrix multiplications may be hard to keep up-> we can name dimensions as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5810ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cfcac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... adding this we can leave out any number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8115bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kalbang\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:780: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return super(Tensor, self).refine_names(names)\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16d7b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a39b572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_namedL torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "print(\"img_namedL\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae04be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#values of tensors are allocated in contiguous chunks of memory by torch.storage unlike Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8dfa185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6db7f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#even though tensor is 2-D, storage is a contiguous array of 6 elements( always 1 dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d0dc6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_storage = points.storage()\n",
    "points_storage[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93b08351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#element in storage is based off offset, stride, and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4524779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accesing element(i,j) =  offset + stride[0] *i + stride[1] * j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e06117d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f586695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every tensor can be moved to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f358219",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e85323fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14e412df",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4ab9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving a tensor object to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "251cd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(points, '../data/ourpoints.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89244a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef52d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
