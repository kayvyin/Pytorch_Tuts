{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b6fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2516ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = imageio.imread('../data/bobby.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433c0d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.shape #H x W x C Pytorch vision modules only work with C X H X W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4372a1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 77,  45,  22],\n",
       "        [ 77,  45,  22],\n",
       "        [ 78,  46,  21],\n",
       "        ...,\n",
       "        [118,  78,  52],\n",
       "        [117,  77,  51],\n",
       "        [116,  76,  51]],\n",
       "\n",
       "       [[ 75,  43,  20],\n",
       "        [ 76,  44,  21],\n",
       "        [ 77,  45,  20],\n",
       "        ...,\n",
       "        [118,  78,  52],\n",
       "        [117,  77,  51],\n",
       "        [116,  76,  50]],\n",
       "\n",
       "       [[ 74,  39,  17],\n",
       "        [ 75,  41,  16],\n",
       "        [ 77,  43,  18],\n",
       "        ...,\n",
       "        [119,  80,  51],\n",
       "        [117,  77,  51],\n",
       "        [116,  76,  50]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[215, 165,  78],\n",
       "        [216, 166,  79],\n",
       "        [217, 167,  80],\n",
       "        ...,\n",
       "        [172, 122,  51],\n",
       "        [174, 124,  53],\n",
       "        [174, 124,  53]],\n",
       "\n",
       "       [[215, 165,  78],\n",
       "        [216, 166,  79],\n",
       "        [217, 167,  80],\n",
       "        ...,\n",
       "        [173, 123,  54],\n",
       "        [174, 124,  55],\n",
       "        [174, 124,  55]],\n",
       "\n",
       "       [[215, 165,  78],\n",
       "        [216, 166,  79],\n",
       "        [217, 167,  80],\n",
       "        ...,\n",
       "        [159, 108,  42],\n",
       "        [158, 107,  41],\n",
       "        [158, 107,  41]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ec1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img_arr) #turn NumPy array to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908c22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = img.permute(2, 0, 1) #permute to C X H X W (Sidenote: changing img will change out as they use same tensor storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe800ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above case is common when we use tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97424e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch is in N X C X H X W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367110a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c958b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a279c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd0b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/image-cats/'\n",
    "filenames=[name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']\n",
    "for i, filename in enumerate(filenames):\n",
    "    img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "    img_t = torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2, 0, 1)\n",
    "    img_t = img_t[:3] #keep first three channels sometimes images add other dimensions\n",
    "    batch[i] = img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "716c7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Networks work best w/ input ranges [0,1] or [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d2817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usually we will have to cast a tensor to floating point and then normalize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e462873",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6791494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4f2fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6118, 0.5961, 0.4863,  ..., 0.5882, 0.5843, 0.6196],\n",
       "          [0.6824, 0.5255, 0.6471,  ..., 0.4706, 0.5333, 0.5412],\n",
       "          [0.4980, 0.6118, 0.4196,  ..., 0.5137, 0.5608, 0.6431],\n",
       "          ...,\n",
       "          [0.4549, 0.5098, 0.5059,  ..., 0.4980, 0.4627, 0.4392],\n",
       "          [0.5059, 0.5098, 0.4824,  ..., 0.4510, 0.4745, 0.4471],\n",
       "          [0.5059, 0.4824, 0.4627,  ..., 0.4431, 0.4745, 0.4706]],\n",
       "\n",
       "         [[0.5451, 0.5294, 0.4275,  ..., 0.5294, 0.5294, 0.5765],\n",
       "          [0.6275, 0.4667, 0.5843,  ..., 0.4118, 0.4784, 0.4863],\n",
       "          [0.4431, 0.5490, 0.3529,  ..., 0.4627, 0.5059, 0.5961],\n",
       "          ...,\n",
       "          [0.3882, 0.4314, 0.4353,  ..., 0.4588, 0.4235, 0.4039],\n",
       "          [0.4353, 0.4353, 0.4157,  ..., 0.4157, 0.4392, 0.4118],\n",
       "          [0.4353, 0.4078, 0.4000,  ..., 0.4039, 0.4314, 0.4353]],\n",
       "\n",
       "         [[0.5059, 0.4824, 0.3843,  ..., 0.5137, 0.5176, 0.5686],\n",
       "          [0.6078, 0.4314, 0.5373,  ..., 0.4000, 0.4667, 0.4745],\n",
       "          [0.4078, 0.5176, 0.3137,  ..., 0.4392, 0.4902, 0.5725],\n",
       "          ...,\n",
       "          [0.3647, 0.4235, 0.4118,  ..., 0.4902, 0.4510, 0.4235],\n",
       "          [0.4235, 0.4235, 0.3843,  ..., 0.4314, 0.4588, 0.4314],\n",
       "          [0.4196, 0.3843, 0.3725,  ..., 0.4235, 0.4510, 0.4549]]],\n",
       "\n",
       "\n",
       "        [[[0.7922, 0.7569, 0.7451,  ..., 0.0510, 0.0510, 0.0471],\n",
       "          [0.7804, 0.7529, 0.7412,  ..., 0.0549, 0.0549, 0.0549],\n",
       "          [0.7765, 0.7569, 0.7373,  ..., 0.0471, 0.0471, 0.0471],\n",
       "          ...,\n",
       "          [0.3647, 0.3216, 0.2980,  ..., 0.1412, 0.1412, 0.1412],\n",
       "          [0.2941, 0.2667, 0.3961,  ..., 0.1412, 0.1412, 0.1451],\n",
       "          [0.3333, 0.4039, 0.3529,  ..., 0.1412, 0.1451, 0.1490]],\n",
       "\n",
       "         [[0.5922, 0.5451, 0.5216,  ..., 0.0353, 0.0353, 0.0314],\n",
       "          [0.5922, 0.5490, 0.5255,  ..., 0.0431, 0.0431, 0.0431],\n",
       "          [0.5961, 0.5608, 0.5255,  ..., 0.0431, 0.0431, 0.0431],\n",
       "          ...,\n",
       "          [0.2235, 0.1765, 0.1529,  ..., 0.1020, 0.1020, 0.1020],\n",
       "          [0.1294, 0.1020, 0.2314,  ..., 0.1020, 0.1020, 0.1059],\n",
       "          [0.1569, 0.2275, 0.1765,  ..., 0.1020, 0.1059, 0.1098]],\n",
       "\n",
       "         [[0.2667, 0.2078, 0.1725,  ..., 0.0235, 0.0235, 0.0196],\n",
       "          [0.2627, 0.2118, 0.1725,  ..., 0.0235, 0.0235, 0.0235],\n",
       "          [0.2627, 0.2196, 0.1725,  ..., 0.0235, 0.0235, 0.0235],\n",
       "          ...,\n",
       "          [0.1216, 0.0745, 0.0471,  ..., 0.0667, 0.0667, 0.0667],\n",
       "          [0.0431, 0.0078, 0.1373,  ..., 0.0667, 0.0667, 0.0706],\n",
       "          [0.0745, 0.1451, 0.0863,  ..., 0.0667, 0.0706, 0.0745]]],\n",
       "\n",
       "\n",
       "        [[[0.9333, 0.9333, 0.9333,  ..., 0.8392, 0.8431, 0.8431],\n",
       "          [0.9333, 0.9333, 0.9333,  ..., 0.8392, 0.8431, 0.8431],\n",
       "          [0.9333, 0.9333, 0.9333,  ..., 0.8392, 0.8431, 0.8431],\n",
       "          ...,\n",
       "          [0.8392, 0.8353, 0.8314,  ..., 0.7333, 0.7451, 0.7569],\n",
       "          [0.8392, 0.8353, 0.8314,  ..., 0.7294, 0.7451, 0.7529],\n",
       "          [0.8392, 0.8353, 0.8314,  ..., 0.7294, 0.7451, 0.7529]],\n",
       "\n",
       "         [[0.7647, 0.7647, 0.7647,  ..., 0.6784, 0.6863, 0.6863],\n",
       "          [0.7647, 0.7647, 0.7647,  ..., 0.6784, 0.6863, 0.6863],\n",
       "          [0.7647, 0.7647, 0.7647,  ..., 0.6784, 0.6863, 0.6863],\n",
       "          ...,\n",
       "          [0.5020, 0.4980, 0.4941,  ..., 0.3922, 0.4039, 0.4157],\n",
       "          [0.5020, 0.4980, 0.4941,  ..., 0.3882, 0.4039, 0.4118],\n",
       "          [0.5020, 0.4980, 0.4941,  ..., 0.3882, 0.4039, 0.4118]],\n",
       "\n",
       "         [[0.5373, 0.5373, 0.5373,  ..., 0.4902, 0.4941, 0.4941],\n",
       "          [0.5373, 0.5373, 0.5373,  ..., 0.4902, 0.4941, 0.4941],\n",
       "          [0.5373, 0.5373, 0.5373,  ..., 0.4902, 0.4941, 0.4941],\n",
       "          ...,\n",
       "          [0.3098, 0.3059, 0.3020,  ..., 0.2510, 0.2667, 0.2824],\n",
       "          [0.3098, 0.3059, 0.3020,  ..., 0.2510, 0.2706, 0.2784],\n",
       "          [0.3098, 0.3059, 0.3020,  ..., 0.2549, 0.2706, 0.2824]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59476dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to normalize is compute mean and std of input and scale so output has 0 mean and unit std each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17df8a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channels = batch.shape[1]\n",
    "n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7430749b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f95eae7c2790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "for c in (n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
